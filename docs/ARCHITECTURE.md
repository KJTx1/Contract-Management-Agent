# System Architecture

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INGESTION PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  OCI Object Storage PDF                                         â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€> Stream PDF bytes to memory                          â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â”œâ”€â”€> Text Extraction (PyPDF2 from bytes)       â”‚
â”‚       â”‚         â”‚         â”‚                                     â”‚
â”‚       â”‚         â”‚         â”œâ”€â”€> Chunking (800 tokens, 100 overlap) â”‚
â”‚       â”‚         â”‚         â”‚         â”‚                           â”‚
â”‚       â”‚         â”‚         â”‚         â”œâ”€â”€> Embedding Generation  â”‚
â”‚       â”‚         â”‚         â”‚         â”‚    (OpenAI/Cohere)         â”‚
â”‚       â”‚         â”‚         â”‚         â”‚         â”‚                 â”‚
â”‚       â”‚         â”‚         â”‚         â”‚         â””â”€â”€> FAISS Index  â”‚
â”‚       â”‚         â”‚         â”‚         â”‚                           â”‚
â”‚       â”‚         â”‚         â”‚         â””â”€â”€> SQLite (chunks table)  â”‚
â”‚       â”‚         â”‚         â”‚                                     â”‚
â”‚       â”‚         â”‚         â””â”€â”€> AI Metadata Extraction (LLM)   â”‚
â”‚       â”‚         â”‚                  â”‚                           â”‚
â”‚       â”‚         â”‚                  â””â”€â”€> SQLite (documents table) â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â””â”€â”€> Store OCI URL (no local storage)          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY PIPELINE (LangGraph)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User Query                                                     â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€> [Node 1] Embed Query                                â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â””â”€â”€> Query Embedding                            â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€> [Node 2] Retrieve Chunks                            â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â”œâ”€â”€> FAISS Similarity Search                    â”‚
â”‚       â”‚         â”‚         â”‚                                     â”‚
â”‚       â”‚         â””â”€â”€> Apply Metadata Filters (SQLite)            â”‚
â”‚       â”‚                   â”‚                                     â”‚
â”‚       â”‚                   â””â”€â”€> Top-K Relevant Chunks            â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€> [Node 3] Combine Context                            â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â””â”€â”€> Build Prompt with Chunks                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€> [Node 4] Generate Answer                            â”‚
â”‚       â”‚         â”‚                                               â”‚
â”‚       â”‚         â””â”€â”€> LLM (GPT-4o-mini)                         â”‚
â”‚       â”‚                   â”‚                                     â”‚
â”‚       â”‚                   â””â”€â”€> Generated Answer                 â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€> [Node 5] Format Output                              â”‚
â”‚                 â”‚                                               â”‚
â”‚                 â””â”€â”€> Answer + Citations + Sources               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Component Details

### Storage Layer

```
data/
â”œâ”€â”€ logistics.db              # SQLite Database
â”‚   â”œâ”€â”€ documents table       # Document metadata
â”‚   â”‚   â”œâ”€â”€ doc_id (PK)
â”‚   â”‚   â”œâ”€â”€ filename
â”‚   â”‚   â”œâ”€â”€ customer_name
â”‚   â”‚   â”œâ”€â”€ doc_type
â”‚   â”‚   â”œâ”€â”€ doc_date
â”‚   â”‚   â”œâ”€â”€ shipment_id
â”‚   â”‚   â””â”€â”€ ... (15+ fields)
â”‚   â”‚
â”‚   â””â”€â”€ chunks table          # Text chunks
â”‚       â”œâ”€â”€ chunk_id (PK)
â”‚       â”œâ”€â”€ doc_id (FK)
â”‚       â”œâ”€â”€ chunk_text
â”‚       â”œâ”€â”€ chunk_embedding_id
â”‚       â””â”€â”€ ... (metadata)
â”‚
â”œâ”€â”€ faiss_index.index         # FAISS Vector Index
â”‚   â””â”€â”€ 1536-dimensional embeddings
â”‚
â””â”€â”€ pdfs/                     # Original PDFs
    â””â”€â”€ {uuid}_{filename}.pdf
```

### Code Structure

```
src/agent/
â”œâ”€â”€ config.py                 # Configuration management
â”‚   â””â”€â”€ Config class
â”‚
â”œâ”€â”€ database.py               # SQLite operations
â”‚   â””â”€â”€ Database class
â”‚       â”œâ”€â”€ insert_document()
â”‚       â”œâ”€â”€ insert_chunk()
â”‚       â”œâ”€â”€ search_chunks()
â”‚       â””â”€â”€ get_stats()
â”‚
â”œâ”€â”€ pdf_processor.py          # PDF processing
â”‚   â””â”€â”€ PDFProcessor class
â”‚       â”œâ”€â”€ extract_text_from_pdf()
â”‚       â”œâ”€â”€ chunk_text()
â”‚       â””â”€â”€ extract_metadata_with_llm()
â”‚
â”œâ”€â”€ vector_operations.py      # FAISS & embeddings
â”‚   â”œâ”€â”€ VectorStore class
â”‚   â”‚   â”œâ”€â”€ add_vectors()
â”‚   â”‚   â””â”€â”€ search()
â”‚   â””â”€â”€ EmbeddingGenerator class
â”‚       â””â”€â”€ generate_embeddings()
â”‚
â”œâ”€â”€ ingestion.py              # Ingestion pipeline
â”‚   â””â”€â”€ DocumentIngestionPipeline class
â”‚       â”œâ”€â”€ ingest_document()
â”‚       â””â”€â”€ ingest_directory()
â”‚
â”œâ”€â”€ rag_pipeline.py           # LangGraph RAG
â”‚   â”œâ”€â”€ RAGPipeline class
â”‚   â”‚   â”œâ”€â”€ embed_query()
â”‚   â”‚   â”œâ”€â”€ retrieve_chunks()
â”‚   â”‚   â”œâ”€â”€ combine_context()
â”‚   â”‚   â”œâ”€â”€ generate_answer()
â”‚   â”‚   â””â”€â”€ format_output()
â”‚   â””â”€â”€ graph (compiled LangGraph)
â”‚
â”œâ”€â”€ cli.py                    # Command-line interface
â”‚   â””â”€â”€ CLI class
â”‚       â”œâ”€â”€ ingest_command()
â”‚       â”œâ”€â”€ query_command()
â”‚       â””â”€â”€ interactive_mode()
â”‚
â””â”€â”€ graph.py                  # Main export
    â””â”€â”€ graph
```

## ğŸ”„ Data Flow

### Ingestion Flow

```
1. OCI Object Storage PDF
   â†“
2. Stream PDF bytes to memory
   â†“
3. Extract Text (PyPDF2 from bytes)
   â†“
4. Split into Chunks
   â”œâ”€> Store in SQLite (chunks table)
   â””â”€> Generate Embeddings
       â†“
5. Add to FAISS Index
   â†“
6. Extract Metadata (LLM)
   â†“
7. Store in SQLite (documents table)
   â†“
8. Store OCI URL (no local copy)
```

### Query Flow

```
1. User Question
   â†“
2. Generate Query Embedding
   â†“
3. Search FAISS (semantic similarity)
   â†“
4. Get Chunk IDs
   â†“
5. Fetch from SQLite (with metadata)
   â†“
6. Apply Filters (customer, date, type)
   â†“
7. Build Context Prompt
   â†“
8. Send to LLM
   â†“
9. Generate Answer
   â†“
10. Format with Citations
    â†“
11. Return to User
```

## ğŸ¯ Key Design Decisions

### 1. SQLite for Metadata
- **Why**: Fast, embedded, SQL queries for filtering
- **Alternative**: NoSQL (MongoDB) - overkill for MVP
- **Migration Path**: PostgreSQL or OCI 23ai

### 2. FAISS for Vectors
- **Why**: Fast, mature, easy to use
- **Alternative**: Pinecone, Weaviate - require external services
- **Migration Path**: OCI 23ai vector capabilities

### 3. LangGraph for Orchestration
- **Why**: Modular, testable, cloud-ready
- **Alternative**: Simple functions - harder to maintain
- **Benefits**: Easy to add nodes, visualize, debug

### 4. PyPDF2 for Extraction
- **Why**: Lightweight, no dependencies
- **Alternative**: pdfplumber, Unstructured.io
- **Limitation**: Text-based PDFs only (no OCR)

### 5. OpenAI for Embeddings & LLM
- **Why**: Best quality, widely available
- **Alternative**: Cohere (supported), local models
- **Cost**: ~$0.01-0.05 per document

## ğŸ”Œ Integration Points

### Current (MVP)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI  â”‚â”€â”€â”€â”€â–¶â”‚  Agent  â”‚â”€â”€â”€â”€â–¶â”‚ CLI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€â–¶ SQLite
                     â”œâ”€â”€â”€â–¶ FAISS
                     â””â”€â”€â”€â–¶ OCI Object Storage
```

### Future (Production)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI  â”‚â”€â”€â”€â”€â–¶â”‚  Agent  â”‚â”€â”€â”€â”€â–¶â”‚ FastAPI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                â”‚
                     â”œâ”€â”€â”€â–¶ OCI 23ai   â”‚
                     â”œâ”€â”€â”€â–¶ OCI Object â”‚
                     â””â”€â”€â”€â–¶ Storage    â”‚
                                      â”‚
                                â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                                â”‚  Web UI   â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Scalability Considerations

### Current Limits
- **Documents**: 100-500 PDFs (FAISS L2 index)
- **Concurrent Users**: 1-10 (single process)
- **Query Latency**: 1-3 seconds
- **Storage**: OCI Object Storage (unlimited)

### Scaling Up
- **More Documents**: FAISS IVF index or migrate to OCI 23ai
- **More Users**: Deploy as FastAPI service with workers
- **Faster Queries**: Batch embedding generation, caching
- **More Storage**: OCI Object Storage

## ğŸ›¡ï¸ Security & Privacy

### Current
- **Data**: OCI Object Storage (cloud)
- **API Keys**: Environment variables
- **Access**: OCI IAM permissions
- **Network**: HTTPS/TLS encrypted

### Production Recommendations
- **Authentication**: JWT tokens
- **Authorization**: Role-based access
- **Encryption**: TLS for API, encrypted storage
- **Audit**: Log all queries and access

## ğŸ”§ Configuration Options

```python
# Embedding
EMBEDDING_PROVIDER = "openai" | "cohere"
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSION = 1536

# Chunking
CHUNK_SIZE = 800          # tokens
CHUNK_OVERLAP = 100       # tokens

# Retrieval
TOP_K = 5                 # results
SIMILARITY_THRESHOLD = 0.7

# LLM
LLM_MODEL = "gpt-4o-mini"
```

## ğŸ“ˆ Performance Metrics

### Ingestion
- **Text Extraction**: ~1-2s per PDF
- **Chunking**: <0.1s per document
- **Embedding**: ~0.5-1s per document
- **LLM Metadata**: ~2-3s per document
- **Total**: ~3-6s per document

### Query
- **Embedding**: ~0.2-0.5s
- **FAISS Search**: <0.1s
- **DB Lookup**: <0.1s
- **LLM Generation**: ~1-2s
- **Total**: ~1.5-3s per query

## ğŸ“ Extension Points

### Add New Document Types
1. Extend `PDFProcessor` for new formats
2. Update metadata schema in `database.py`
3. Add type-specific extraction logic

### Add New Retrieval Methods
1. Implement in `RAGPipeline`
2. Add as new LangGraph node
3. Combine with existing retrieval

### Add New Interfaces
1. FastAPI: Create REST endpoints
2. Streamlit: Build web UI
3. Slack Bot: Integrate messaging

This architecture provides a solid foundation for the MVP while maintaining clear paths for production scaling! ğŸš€

